{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [1:24:47<00:00,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1, 972)) :\n",
    "    table_url = 'https://kin.naver.com/best/listaha.nhn?page=' + str(i)\n",
    "    table_request = requests.get(table_url)\n",
    "    table_soup = BeautifulSoup(table_request.text, 'html.parser')\n",
    "    url_list = table_soup.find('tbody', {'id' : 'au_board_list'}).find_all('td', {'class' : 'title'})\n",
    "    for link in url_list :\n",
    "        doc_url = 'https://kin.naver.com' + link.find('a').get('href')\n",
    "        doc_request = requests.get(doc_url)\n",
    "        # 네이버 서버에서 튕기는 것을 막기 위해 request마다 2초씩 term\n",
    "#         time.sleep(2)\n",
    "        \n",
    "        doc_soup = BeautifulSoup(doc_request.text, 'html.parser')\n",
    "        \n",
    "        # doc_infomation\n",
    "        if not doc_soup.find('ul', {'class' : 'location'}) :\n",
    "            continue   # 삭제된 게시물은 continue\n",
    "        title = link.text.strip()\n",
    "        category = doc_soup.find('ul', {'class' : 'location'}).find_all('a')[2].text.strip()\n",
    "        q_id = doc_url.split('=')[-1]\n",
    "        \n",
    "        # question part\n",
    "        question = doc_soup.find('div', {'class' : 'end_question'})\n",
    "        temp_dict = {}\n",
    "        if question :\n",
    "            temp_dict['title'] = title\n",
    "            temp_dict['text'] = question.find('div', {'class' : '_endContentsText'}).text.strip()\n",
    "            temp_dict['category'] = category\n",
    "            temp_dict['datetime'] = [temp for temp in map(lambda x : re.search(r'(\\d{4}.\\d{2}.\\d{2})', x), map(lambda x : x.text, question.find_all('dd', {'class' : 'date'}))) if temp is not None][0].group(1)\n",
    "            temp_dict['q_id'] = q_id\n",
    "            temp_dict['qa'] = 0   # 0 : question, 1 : adopted answser, 2 : other answers\n",
    "            data.append(temp_dict)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #answer part\n",
    "        answers = doc_soup.find_all('div', {'class' : 'end_answer'})\n",
    "        for answer in answers :\n",
    "            temp_dict = {}\n",
    "            try:\n",
    "                if '삭제' in answer.find('h3').text.strip() :\n",
    "                    continue\n",
    "                else :\n",
    "                    if answer.find('div', {'class' : 'end_title_ico adopt_question'}) :\n",
    "                        temp_dict['qa'] = 1\n",
    "                    else :\n",
    "                        temp_dict['qa'] = 2\n",
    "                    temp_dict['title'] = title\n",
    "                    temp_dict['text'] = answer.find('div', {'class' : '_endContentsText'}).text.strip()\n",
    "                    temp_dict['category'] = category\n",
    "                    temp_dict['datetime'] = answer.find('div', {'class' : 'end_date'}).text.strip()\n",
    "                    temp_dict['q_id'] = q_id\n",
    "                    data.append(temp_dict)\n",
    "            except :\n",
    "                continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중간정리\n",
    " - time.sleep 안해도 잘 스크래핑 됨\n",
    " - 리스트를 나누지 말고 dict 타입으로 instance 하나씩 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df.to_csv('raw_kin_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('raw_kin_data.csv', na_filter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s) :\n",
    "    # only include alphanumerics and Korean\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣,!?\\'\\`]\", \" \", s)\n",
    "    \n",
    "    # insert spaces in words with apostrophes\n",
    "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "    \n",
    "    # insert spaces in special characters\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\?\", \" \\? \", s)\n",
    "    \n",
    "    # only include alphanumerics and Korean again\n",
    "    s = re.sub(r\"[^A-Za-z0-9가-힣(),!?\\'\\`]\", \" \", s)\n",
    "    \n",
    "    # reduce multiple spaces to single spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = raw_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = [clean_str(x) for x in raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_model = Word2Vec(list(map(lambda x : x.split(), clean_text)), size = 300, min_count = 10, workers = 12, negative = 10, iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_model.save('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Word2Vec.load('word2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character별로 초성, 중성, 종성 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hgtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = embedding_model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bag = ''.join(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bag =re.sub('[^가-힣]', '', char_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = []\n",
    "middle = []\n",
    "end = []\n",
    "for s in char_bag :\n",
    "    if s :\n",
    "        f, m, e = hgtk.letter.decompose(s)\n",
    "        first.append(f)\n",
    "        middle.append(m)\n",
    "        end.append(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = list(set(first))\n",
    "middle = list(set(middle))\n",
    "end = list(set(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dict = {}\n",
    "for i, v in enumerate(first) :\n",
    "    first_dict[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_dict = {}\n",
    "for i, v in enumerate(middle) :\n",
    "    middle_dict[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dict = {}\n",
    "for i, v in enumerate(end) :\n",
    "    end_dict[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hangul_vocab = [word for word in word_vocab if hgtk.checker.is_hangul(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = list(map(lambda x : len(x), hangul_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word_list) :\n",
    "    return[first_dict[word_list[0]], middle_dict[word_list[1]], end_dict[word_list[2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67717/67717 [1:39:50<00:00, 11.30it/s]  \n"
     ]
    }
   ],
   "source": [
    "# train_text = list()\n",
    "# for text in tqdm(clean_text) :\n",
    "#     train_text.append([word for word in text.split() if word in hangul_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train_text.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_text.pkl', 'rb') as f :\n",
    "    train_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ord() expected a character, but string of length 3 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-05e599348290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mhgtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-05e599348290>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mhgtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpad_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/hgtk/letter.py\u001b[0m in \u001b[0;36mdecompose\u001b[0;34m(hangul_letter)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhangul_letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhangul_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhangul_letter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mcho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompose_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/hgtk/letter.py\u001b[0m in \u001b[0;36mhangul_index\u001b[0;34m(letter)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhangul_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mFIRST_HANGUL_UNICODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 3 found"
     ]
    }
   ],
   "source": [
    "tokens = list()\n",
    "lengths = list()\n",
    "targets = list()\n",
    "for word in train_text :\n",
    "    temp = list(map(lambda x : hgtk.letter.decompose(x), word))\n",
    "    temp = list(map(get_index, temp))\n",
    "    pad_length = max(length) - len(word)\n",
    "    temp = np.pad(temp, [[0, pad_length], [0, 0]], 'constant', constant_values = 0).tolist()\n",
    "    tokens.append(temp)\n",
    "    length.append(len(word))\n",
    "    targets.append(embedding_model.wv.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "embedding_size = 100\n",
    "num_sample = 32\n",
    "epochs = 3\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "train_y = tf.placeholder(tf.int32, shape = [batch_size, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_embeddings = tf.Variable(\n",
    "    tf.random_uniform([len(first), embedding_size], -1.0, 1.0))\n",
    "\n",
    "middle_embeddings = tf.Variable(\n",
    "    tf.random_uniform([len(middle), embedding_size], -1.0, 1.0))\n",
    "\n",
    "end_embeddings = tf.Variable(\n",
    "    tf.random_uniform([len(end), embedding_size], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess :\n",
    "    train_inputs.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index = list(map(lambda x : first.index(x), train_input))\n",
    "first_index = list(map(lambda x : first.index(x), ['ㅁ', 'ㅈ', 'ㅂ', 'ㅁ', 'ㅅ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_embed = tf.nn.embedding_lookup(first_embeddings, first_index)\n",
    "middle_embed = tf.nn.embedding_lookup(middle_embeddings, middle_index)\n",
    "end_embed = tf.nn.embedding_lookup(end_embeddings, end_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
